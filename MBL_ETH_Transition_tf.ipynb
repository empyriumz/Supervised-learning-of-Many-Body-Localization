{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network for MBL-ETH Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sklearn\n",
    "# suppress tensorflow compilation warnings\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "# tf.set_random_seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format ='retina' #plot high-res img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use entanglement spectrum to feed the network\n",
    "$j=5.0$ data represent typical the thermal phase, $j=0.1$ for the MBL phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6560, 128) (6560, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load the example data set with L=12\n",
    "# p1 represents the localized MBL phase while p2 is ETH phase\n",
    "x = np.load('phase_1.npz')\n",
    "p1 = x['ent']\n",
    "x = np.load('phase_2.npz')\n",
    "p2 = x['ent']\n",
    "print(p1.shape,p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is only useful if you run the en_spectrum.py script to generate your own data set\n",
    "L = 12\n",
    "runs = 4 # number of samples generated\n",
    "loaded = np.load('1en_spectrum_L=12_j=0.1.npz')\n",
    "\n",
    "dim= len(loaded['ent']) # the dim is determined by the cut position\n",
    "x = np.zeros((runs,dim,2**(int(L//2+1))), dtype='float64')\n",
    "for i in range(runs):\n",
    "    filename = \"\"+str(i+1)+\"en_spectrum_L=12_j=0.1.npz\"\n",
    "    loaded = np.load(filename)\n",
    "    x[i] = loaded['ent']\n",
    "p1 = np.concatenate((x[:])) \n",
    "\n",
    "runs = 4\n",
    "x = np.zeros((runs,dim,2**(int(L//2+1))), dtype='float64')\n",
    "for i in range(runs):\n",
    "    filename = \"\"+str(i+1)+\"en_spectrum_L=12_j=5.0.npz\"\n",
    "    loaded = np.load(filename)\n",
    "    x[i] = loaded['ent']\n",
    "p2 = np.concatenate((x[:])) \n",
    "\n",
    "# cast the original data to single precision to speed up\n",
    "p1 = p1.astype('float32')\n",
    "p2 = p2.astype('float32')\n",
    "print(p1.shape, p2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use wavefunction (absolute value squared) as raw data to feed the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the wavefunction data\n",
    "x = np.load('phase_1.npz')\n",
    "p1 = x['wav']\n",
    "x = np.load('phase_2.npz')\n",
    "p2 = x['wav']\n",
    "print(p1.shape,p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is only useful if you run the en_spectrum.py script to generate your own data set\n",
    "L = 12\n",
    "runs = 4\n",
    "dim = int(2**(L-1))\n",
    "x = np.zeros((runs, 200, dim), dtype='float64')\n",
    "for i in range(runs):\n",
    "    filename = \"\"+str(i+1)+\"en_spectrum_L=12_j=0.1.npz\"\n",
    "    loaded = np.load(filename)\n",
    "    wav = loaded['wave']\n",
    "    abswav = np.multiply(wav,np.conj(wav))\n",
    "    x[i] = np.multiply(wav,np.conj(wav))\n",
    "p1 = np.concatenate((x[:]))\n",
    "\n",
    "runs = 4\n",
    "x = np.zeros((runs, 200, dim), dtype='float64')\n",
    "for i in range(runs):\n",
    "    filename = \"\"+str(i+1)+\"en_spectrum_L=12_j=5.0.npz\"\n",
    "    loaded = np.load(filename)\n",
    "    wav = loaded['wave']\n",
    "    abswav = np.multiply(wav,np.conj(wav))\n",
    "    x[i] = np.multiply(wav,np.conj(wav))\n",
    "p2 = np.concatenate((x[:]))\n",
    "\n",
    "p1 = p1.astype('float32')\n",
    "p2 = p2.astype('float32')\n",
    "print(p1.shape, p2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label two phases and partition trainning and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10496, 128) (10496, 2) (2624, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#combine entanglement spectrum of different system sizes with 2 labels\n",
    "num_classes = 2\n",
    "l1 = np.ones(p1.shape[0],dtype=np.int8)\n",
    "l2 = np.zeros(p2.shape[0],dtype=np.int8)\n",
    "ph1 = np.column_stack((l1,p1))\n",
    "ph2 = np.column_stack((l2,p2))\n",
    "# gathering all the data\n",
    "dat = np.concatenate((ph1,ph2))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dat[:,1:], dat[:,:1], test_size=0.2, random_state=1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes)\n",
    "print(X_train.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "\n",
    "def create_model():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add a normalization layer to improve robustness\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.95, epsilon=0.001))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(400,input_shape=(X_test.shape[1],), activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    # apply dropout with rate 0.4\n",
    "    model.add(Dropout(0.4))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # compile the model with loss function and optimizer\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10496 samples, validate on 2624 samples\n",
      "Epoch 1/10\n",
      "10496/10496 [==============================] - 2s 203us/sample - loss: 0.4057 - accuracy: 0.8096 - val_loss: 2.5493 - val_accuracy: 0.5252\n",
      "Epoch 2/10\n",
      "10496/10496 [==============================] - 1s 58us/sample - loss: 0.1958 - accuracy: 0.9224 - val_loss: 0.1499 - val_accuracy: 0.9379\n",
      "Epoch 3/10\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 0.1444 - accuracy: 0.9424 - val_loss: 0.1340 - val_accuracy: 0.9466\n",
      "Epoch 4/10\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 0.1150 - accuracy: 0.9516 - val_loss: 0.1458 - val_accuracy: 0.9398\n",
      "Epoch 5/10\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 0.0990 - accuracy: 0.9603 - val_loss: 0.1499 - val_accuracy: 0.9398\n",
      "Epoch 6/10\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 0.0807 - accuracy: 0.9705 - val_loss: 0.1397 - val_accuracy: 0.9459\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 150\n",
    "epochs = 10\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#early termination to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# create the deep neural net\n",
    "model_DNN=create_model()\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history=model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Save trained model which can be trained later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#save the model and continue training\n",
    "model_DNN.save('my_model.h5')\n",
    "from tensorflow.keras.models import load_model\n",
    "model_DNN = load_model('my_model.h5')\n",
    "\n",
    "\n",
    "# training parameters\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# create the deep neural net\n",
    "# model_DNN=compile_model()\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history = model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
    "\n",
    "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bf7714a24a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model_DNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning\n",
    "Use scikit-learn to grid search the batch size and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [20, 50, 100]\n",
    "epochs = [5, 10, 15]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
